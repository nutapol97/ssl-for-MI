{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f5c784b-e99a-40e1-9453-a7d754508a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "148b7e57-a8b7-4695-ad8e-775cb0fe0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /opt/conda/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.9/site-packages (from mne) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from mne) (1.7.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from mne) (3.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from mne) (21.3)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/conda/lib/python3.9/site-packages (from mne) (1.6.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from mne) (4.62.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from mne) (3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from pooch>=1.5->mne) (2.27.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from pooch>=1.5->mne) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->mne) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->mne) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (4.28.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mne) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a06ff5a-770e-490f-8b8a-2341de2d96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c352b33-cc7e-47b9-bb02-cc55034373c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: warmup-scheduler in /opt/conda/lib/python3.9/site-packages (0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install warmup-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d3fe08b-8f7c-4439-a50a-06ac2d537926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4a67bea-12f3-413f-a6ba-9c00854fe16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from net import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import tqdm\n",
    "import mit_utils as utils\n",
    "# import analytics\n",
    "import time\n",
    "import os, shutil\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24c5c92a-1dfe-458f-84ae-861d039e98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=64, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        return events , event_id\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def get_X_y(self):\n",
    "        if self.epochs is None:\n",
    "            events , event_id=self.create_epochs()\n",
    "        self.X = self.epochs.get_data()\n",
    "        self.y = self.epochs.events[:, -1]\n",
    "        return self.X, self.y\n",
    "    \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"edf\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_edf( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "        \n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='valid_loss')\n",
    "    plt.title('loss {}'.format(iter))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e1694-ae3c-4a9f-9ab9-19fa54692d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba306842-10d9-4f07-ac2f-f2a39c09f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def save_ckpt(state, is_best, model_save_dir, message='best_w.pth'):\n",
    "    current_w = os.path.join(model_save_dir, 'latest_w.pth')\n",
    "    best_w = os.path.join(model_save_dir, message)\n",
    "    torch.save(state, current_w)\n",
    "    if is_best: shutil.copyfile(current_w, best_w)\n",
    "\n",
    "def transform(x, mode):\n",
    "    x_ = x.cpu().numpy()\n",
    "\n",
    "    Trans = Transform()\n",
    "    if mode == 'time_warp':\n",
    "        pieces = random.randint(5,20)\n",
    "        stretch = random.uniform(1.5,4)\n",
    "        squeeze = random.uniform(0.25,0.67)\n",
    "        x_ = Trans.time_warp(x_, 100, pieces, stretch, squeeze)\n",
    "    elif mode == 'noise':\n",
    "        factor = random.uniform(10,20)\n",
    "        x_ = Trans.add_noise_with_SNR(x_,factor)\n",
    "    elif mode == 'scale':\n",
    "        x_ = Trans.scaled(x_,[0.3,3])\n",
    "    elif mode == 'negate':\n",
    "        x_ = Trans.negate(x_)\n",
    "    elif mode == 'hor_flip':\n",
    "        x_ = Trans.hor_filp(x_)\n",
    "        \n",
    "    elif mode == 'permute':\n",
    "        pieces = random.randint(5,20)\n",
    "        x_ = Trans.permute(x_,pieces)\n",
    "        \n",
    "    elif mode == 'cutout_resize':\n",
    "        pieces = random.randint(5, 20)\n",
    "        x_ = Trans.cutout_resize(x_, pieces)\n",
    "    elif mode == 'cutout_zero':\n",
    "        pieces = random.randint(5, 20)\n",
    "        x_ = Trans.cutout_zero(x_, pieces)\n",
    "    elif mode == 'crop_resize':\n",
    "        size = random.uniform(0.25,0.75)\n",
    "        x_ = Trans.crop_resize(x_, size)\n",
    "    elif mode == 'move_avg':\n",
    "        n = random.randint(3, 10)\n",
    "        x_ = Trans.move_avg(x_,n, mode=\"same\")\n",
    "    #     to test\n",
    "    elif mode == 'lowpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff = random.uniform(5,20)\n",
    "        x_ = Trans.lowpass_filter(x_, order, [cutoff])\n",
    "    elif mode == 'highpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff = random.uniform(5, 10)\n",
    "        x_ = Trans.highpass_filter(x_, order, [cutoff])\n",
    "    elif mode == 'bandpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff_l = random.uniform(1, 5)\n",
    "        cutoff_h = random.uniform(20, 40)\n",
    "        cutoff = [cutoff_l, cutoff_h]\n",
    "        x_ = Trans.bandpass_filter(x_, order, cutoff)\n",
    "\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    x_ = x_.copy()\n",
    "    x_ = x_[:,None,:]\n",
    "    return x_\n",
    "\n",
    "def comtrast_loss(x, criterion):\n",
    "    LARGE_NUM = 1e9\n",
    "    temperature = 0.1\n",
    "    x = F.normalize(x, dim=-1)\n",
    "\n",
    "    num = int(x.shape[0] / 2)\n",
    "    hidden1, hidden2 = torch.split(x, num)\n",
    "\n",
    "\n",
    "    hidden1_large = hidden1\n",
    "    hidden2_large = hidden2\n",
    "    labels = torch.arange(0,num).to(device)\n",
    "    masks = F.one_hot(torch.arange(0,num), num).to(device)\n",
    "\n",
    "\n",
    "    logits_aa = torch.matmul(hidden1, hidden1_large.T) / temperature\n",
    "    logits_aa = logits_aa - masks * LARGE_NUM\n",
    "    logits_bb = torch.matmul(hidden2, hidden2_large.T) / temperature\n",
    "    logits_bb = logits_bb - masks * LARGE_NUM\n",
    "    logits_ab = torch.matmul(hidden1, hidden2_large.T) / temperature\n",
    "    logits_ba = torch.matmul(hidden2, hidden1_large.T) / temperature\n",
    "    # print(labels)\n",
    "    #\n",
    "    # print(torch.cat([logits_ab, logits_aa], 1).shape)\n",
    "\n",
    "    loss_a = criterion(torch.cat([logits_ab, logits_aa], 1),\n",
    "        labels)\n",
    "    loss_b = criterion(torch.cat([logits_ba, logits_bb], 1),\n",
    "        labels)\n",
    "    loss = torch.mean(loss_a + loss_b)\n",
    "    return loss, labels, logits_ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e20e1b7-3be3-4c2e-bd06-ae16476baacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed1d.pth',\n",
    "}\n",
    "\n",
    "dp_rate = 0\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=33, stride=stride,\n",
    "                     padding=16, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes*2)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn0(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "            # residual = torch.cat((residual,residual),1)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(inplanes)\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=33, bias=False, padding=16)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=65, stride=stride,\n",
    "                               padding=32, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn0(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        # out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "            # residual = torch.cat((residual, residual), 1)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, classification, num_classes=2):\n",
    "        self.inplanes = 12\n",
    "        self.classification = classification\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, self.inplanes, kernel_size=33, stride=1, padding=16,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(self.inplanes, self.inplanes, kernel_size=33, stride=2, padding=16,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(self.inplanes)\n",
    "        self.downsample = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(self.inplanes, self.inplanes, kernel_size=33, stride=1, padding=16,\n",
    "                               bias=False)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=2)\n",
    "        # self.layer5 = self._make_layer(block, self.inplanes, layers[4], stride=2)\n",
    "        self.bn_final = nn.BatchNorm1d(96*2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(2)\n",
    "        self.fc1 = nn.Linear(96*4, 384)\n",
    "        self.bn3 = nn.BatchNorm1d(384)\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.bn4 = nn.BatchNorm1d(192)\n",
    "        self.fc3 = nn.Linear(192, 5)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.classification:\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            # x = self.softmax(x)\n",
    "        else :\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            # x = self.maxpool(x)\n",
    "            out = self.conv2(x)\n",
    "            out = self.bn2(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.conv3(out)\n",
    "            residual = self.downsample(x)\n",
    "            #print('residual : {}'.format(residual.shape))\n",
    "            #print('out : {}'.format(out.shape))\n",
    "            out += residual\n",
    "            x = self.relu(out)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            # x = self.layer5(x)\n",
    "            x = self.bn_final(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [ 2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41a1ed4f-a45c-4f32-ae7d-a7b002901dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_noise(self, signal, noise_amount):\n",
    "        \"\"\"\n",
    "        adding noise\n",
    "        \"\"\"\n",
    "        signal = signal.T\n",
    "        noise = (0.4 ** 0.5) * np.random.normal(1, noise_amount, np.shape(signal)[0])\n",
    "        noise = noise[:,None]\n",
    "        noised_signal = signal + noise\n",
    "        noised_signal = noised_signal.T\n",
    "        # print(noised_signal.shape)\n",
    "        return noised_signal\n",
    "\n",
    "    def add_noise_with_SNR(self,signal, noise_amount):\n",
    "        \"\"\"\n",
    "        adding noise\n",
    "        created using: https://stackoverflow.com/a/53688043/10700812\n",
    "        \"\"\"\n",
    "        signal = signal[0]\n",
    "        target_snr_db = noise_amount  # 20\n",
    "        x_watts = signal ** 2  # Calculate signal power and convert to dB\n",
    "        sig_avg_watts = np.mean(x_watts)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)  # Calculate noise then convert to watts\n",
    "        noise_avg_db = sig_avg_db - target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts),\n",
    "                                       len(x_watts))  # Generate an sample of white noise\n",
    "        noised_signal = signal + noise_volts  # noise added signal\n",
    "        noised_signal = noised_signal[None,:]\n",
    "        # print(noised_signal.shape)\n",
    "\n",
    "        return noised_signal\n",
    "\n",
    "    def scaled(self,signal, factor_list):\n",
    "        \"\"\"\"\n",
    "        scale the signal\n",
    "        \"\"\"\n",
    "        factor = round(np.random.uniform(factor_list[0],factor_list[1]),2)\n",
    "        signal[0] = 1 / (1 + np.exp(-signal[0]))\n",
    "        # print(signal.max())\n",
    "        return signal\n",
    "\n",
    "    def negate(self,signal):\n",
    "        \"\"\"\n",
    "        negate the signal\n",
    "        \"\"\"\n",
    "        signal[0] = signal[0] * (-1)\n",
    "        return signal\n",
    "\n",
    "    def hor_filp(self,signal):\n",
    "        \"\"\"\n",
    "        flipped horizontally\n",
    "        \"\"\"\n",
    "        hor_flipped = np.flip(signal,axis=1)\n",
    "        return hor_flipped\n",
    "\n",
    "\n",
    "\n",
    "    def cutout_resize(self,signal,pieces):\n",
    "        \"\"\"\n",
    "                signal: numpy array (batch x window)\n",
    "                pieces: number of segments along time\n",
    "                cutout 1 piece\n",
    "                \"\"\"\n",
    "        signal = signal.T\n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist())  # 向上取整\n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "        import random\n",
    "        sequence = []\n",
    "\n",
    "        cutout = random.randint(0, pieces)\n",
    "        # print(cutout)\n",
    "        # sequence1 = list(range(0, cutout))\n",
    "        # sequence2 = list(range(int(cutout + 1), pieces))\n",
    "        # sequence = np.hstack((sequence1, sequence2))\n",
    "        for i in range(pieces):\n",
    "            if i == cutout:\n",
    "                pass\n",
    "            else:\n",
    "                sequence.append(i)\n",
    "        # print(sequence)\n",
    "\n",
    "        cutout_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                     (pieces, piece_length)).tolist()\n",
    "\n",
    "        tail = signal[(np.shape(signal)[0] // pieces * pieces):]\n",
    "\n",
    "        cutout_signal = np.asarray(cutout_signal)[sequence]\n",
    "\n",
    "        cutout_signal = np.hstack(cutout_signal)\n",
    "        cutout_signal = np.concatenate((cutout_signal, tail[:, 0]), axis=0)\n",
    "\n",
    "        cutout_signal = cv2.resize(cutout_signal, (1, 3072), interpolation=cv2.INTER_LINEAR)\n",
    "        cutout_signal = cutout_signal.T\n",
    "\n",
    "\n",
    "        return cutout_signal\n",
    "\n",
    "    def cutout_zero(self,signal,pieces):\n",
    "        \"\"\"\n",
    "                signal: numpy array (batch x window)\n",
    "                pieces: number of segments along time\n",
    "                cutout 1 piece\n",
    "                \"\"\"\n",
    "        signal = signal.T\n",
    "        ones = np.ones((np.shape(signal)[0],np.shape(signal)[1]))\n",
    "        # print(ones.shape)\n",
    "        # assert False\n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist())  # 向上取整\n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "\n",
    "\n",
    "        cutout = random.randint(1, pieces)\n",
    "        cutout_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                     (pieces, piece_length)).tolist()\n",
    "        ones_pieces = np.reshape(ones[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                   (pieces, piece_length)).tolist()\n",
    "        tail = signal[(np.shape(signal)[0] // pieces * pieces):]\n",
    "\n",
    "        cutout_signal = np.asarray(cutout_signal)\n",
    "        ones_pieces = np.asarray(ones_pieces)\n",
    "        for i in range(pieces):\n",
    "            if i == cutout:\n",
    "                ones_pieces[i]*=0\n",
    "\n",
    "        cutout_signal = cutout_signal * ones_pieces\n",
    "        cutout_signal = np.hstack(cutout_signal)\n",
    "        cutout_signal = np.concatenate((cutout_signal, tail[:, 0]), axis=0)\n",
    "        cutout_signal = cutout_signal[:,None]\n",
    "        cutout_signal = cutout_signal.T\n",
    "\n",
    "        return cutout_signal\n",
    "    # mic\n",
    "    \n",
    "\n",
    "    def move_avg(self,a,n, mode=\"same\"):\n",
    "        # a = a.T\n",
    "\n",
    "        result = np.convolve(a[0], np.ones((n,)) / n, mode=mode)\n",
    "        return result[None,:]\n",
    "\n",
    "    def bandpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        w2 = 2 * cutoff[1] / int(fs)\n",
    "        b, a = signal.butter(order, [w1, w2], btype='bandpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def lowpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        # w2 = 2 * cutoff[1] / fs\n",
    "        b, a = signal.butter(order, w1, btype='lowpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def highpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        # w2 = 2 * cutoff[1] / fs\n",
    "        b, a = signal.butter(order, w1, btype='highpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def time_warp(self,signal, sampling_freq, pieces, stretch_factor, squeeze_factor):\n",
    "        \"\"\"\n",
    "        signal: numpy array (batch x window)\n",
    "        sampling freq\n",
    "        pieces: number of segments along time\n",
    "        stretch factor\n",
    "        squeeze factor\n",
    "        \"\"\"\n",
    "        signal = signal.T\n",
    "\n",
    "        total_time = np.shape(signal)[0] // sampling_freq\n",
    "        segment_time = total_time / pieces\n",
    "        sequence = list(range(0, pieces))\n",
    "        stretch = np.random.choice(sequence, math.ceil(len(sequence) / 2), replace=False)\n",
    "        squeeze = list(set(sequence).difference(set(stretch)))\n",
    "        initialize = True\n",
    "        for i in sequence:\n",
    "            orig_signal = signal[int(i * np.floor(segment_time * sampling_freq)):int(\n",
    "                (i + 1) * np.floor(segment_time * sampling_freq))]\n",
    "            orig_signal = orig_signal.reshape(np.shape(orig_signal)[0],64, 1)\n",
    "            if i in stretch:\n",
    "                output_shape = int(np.ceil(np.shape(orig_signal)[0] * stretch_factor))\n",
    "                new_signal = cv2.resize(orig_signal, (1, output_shape), interpolation=cv2.INTER_LINEAR)\n",
    "                if initialize == True:\n",
    "                    time_warped = new_signal\n",
    "                    initialize = False\n",
    "                else:\n",
    "                    time_warped = np.vstack((time_warped, new_signal))\n",
    "            elif i in squeeze:\n",
    "                output_shape = int(np.ceil(np.shape(orig_signal)[0] * squeeze_factor))\n",
    "                new_signal = cv2.resize(orig_signal, (1, output_shape), interpolation=cv2.INTER_LINEAR)\n",
    "                if initialize == True:\n",
    "                    time_warped = new_signal\n",
    "                    initialize = False\n",
    "                else:\n",
    "                    time_warped = np.vstack((time_warped, new_signal))\n",
    "        time_warped = cv2.resize(time_warped, (1,3072), interpolation=cv2.INTER_LINEAR)\n",
    "        time_warped = time_warped.T\n",
    "        return time_warped\n",
    "    \n",
    "    \n",
    "    def crop_resize(self, signal, size):\n",
    "        signal = signal.T\n",
    "        size = signal.shape[0] * size\n",
    "        size = int(size)\n",
    "        start = random.randint(0, signal.shape[0]-size)\n",
    "        crop_signal = signal[start:start + size,:]\n",
    "        # print(crop_signal.shape)\n",
    "\n",
    "        crop_signal = cv2.resize(crop_signal, (1, 640), interpolation=cv2.INTER_LINEAR)\n",
    "        # print(crop_signal.shape)\n",
    "        crop_signal = crop_signal.T\n",
    "        return crop_signal\n",
    "    \n",
    "    \n",
    "    def permute(self,signal, pieces):\n",
    "        \"\"\"\n",
    "        signal: numpy array (batch x window)\n",
    "        pieces: number of segments along time\n",
    "        \"\"\"\n",
    "        #print(\"signal shape : {}\".format(signal.shape))\n",
    "        #print(\"pieces : {}\".format(pieces))\n",
    "        signal = signal.T\n",
    "        #print(\"signal shape T : {}\".format(signal.shape))\n",
    "        \n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist()) #向上取整\n",
    "        #print(\"pieces : {}\".format(pieces))\n",
    "        \n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "        #print(\"piece_length : {}\".format(piece_length))\n",
    "        \n",
    "        \n",
    "        sequence = list(range(0, pieces))\n",
    "        np.random.shuffle(sequence)\n",
    "#         #print(\"sequence : {}\".format(sequence))\n",
    "        \n",
    "#         #print(\"signal[:(np.shape(signal)[0] // pieces * pieces)].shape : {}\".format(\n",
    "#             signal[:(np.shape(signal)[0] // pieces * pieces)].shape)\n",
    "#              )\n",
    "        \n",
    "#         print(\"(pieces, piece_length) : {0} , {1}\".format(pieces, piece_length)  \n",
    "#              )\n",
    "        \n",
    "#         print(\"(pieces x piece_length) : {} \".format(pieces*piece_length))\n",
    "                          \n",
    "        \n",
    "        permuted_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces*pieces )],(pieces*2, piece_length)).tolist()\n",
    "\n",
    "        \n",
    "        tail = signal[(np.shape(signal)[0] // pieces * pieces):]\n",
    "        permuted_signal = np.asarray(permuted_signal)[sequence]\n",
    "        permuted_signal = np.concatenate(permuted_signal, axis=0)\n",
    "        permuted_signal = np.concatenate((permuted_signal,tail[:,0]), axis=0)\n",
    "        permuted_signal = permuted_signal[:,None]\n",
    "        permuted_signal = permuted_signal.T\n",
    "        return permuted_signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e917af6-f1c7-4361-b4ec-ace0d684544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Apply filter.\n",
      "Filtering raw data in 294 contiguous segments\n",
      "Setting up band-pass filter from 0.05 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.05\n",
      "- Lower transition bandwidth: 0.05 Hz (-6 dB cutoff frequency: 0.03 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 10561 samples (66.006 sec)\n",
      "\n",
      ">>> Create Epochs.\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "4410 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4410 events and 641 original time points ...\n",
      "14 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[    672,       0,       1],\n",
       "        [   2000,       0,       0],\n",
       "        [   3328,       0,       0],\n",
       "        ...,\n",
       "        [5800560,       0,       1],\n",
       "        [5801872,       0,       0],\n",
       "        [5803184,       0,       0]]),\n",
       " {'T1': 0, 'T2': 1})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = '1.0.0'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "runs = [3, 4, 7, 8, 11, 12]\n",
    "subjects = [i for i in range(1, 50)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "# apply filter\n",
    "freq = (0.05, 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.raw_ica()\n",
    "eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcca7d47-2ced-4852-81c9-d4e921b68fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4396, 64, 641) (4396,)\n",
      "(4396, 1, 641)\n",
      "(4396, 1, 641)\n",
      "(4396, 2, 641)\n",
      "(4396, 2, 640)\n"
     ]
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    " \n",
    "#X = X[:, np.newaxis,:,:]\n",
    "X.shape\n",
    "\n",
    "X2 = X[:,  7:8, :] \n",
    "print(X2.shape)\n",
    "\n",
    "X3= X[:,  13:14, :]\n",
    "print(X3.shape)\n",
    "X4 = np.concatenate((X2,X3), axis=1)\n",
    "print(X4.shape)\n",
    "X = X4\n",
    "X=X[:,:,:640]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aead17ed-830d-4cdb-86f7-0295d2dfceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3bdda6f-001a-4baf-8f15-0417d4f44c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(classification=True).to(device)\n",
    "#net = nn.DataParallel(net).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "batch_size = 512\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1 * (batch_size / 64), momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "epochs = 70\n",
    "lr_schduler = CosineAnnealingLR(optimizer, T_max=epochs - 10, eta_min=0.05)#default =0.07\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=10, after_scheduler=lr_schduler)\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "scheduler_warmup.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc60be98-3102-47a7-9f7b-00cfe85d1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3077, 2, 640) (3077,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "      \n",
    "x_train = torch.tensor(x_train, dtype=torch.float).to(device)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9c0f955-fa2f-44b1-a70e-c406ab100845",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = ['R', 'L']\n",
    "val_acc_list = []\n",
    "n_train_samples = x_train.shape[0]\n",
    "iter_per_epoch = n_train_samples // batch_size + 1\n",
    "best_acc = -1\n",
    "err = []\n",
    "best_err = 1\n",
    "margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ed86e63-47b8-407c-85e3-d68729c195f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "model_name = 'resnet17'\n",
    "model_save_dir = '%s/%s_%s' % (log_dir, model_name, time.strftime(\"%m%d%H%M\"))\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "log_templete = {\"acc\": None,\n",
    "                    \"cm\": None,\n",
    "                    \"f1\": None,\n",
    "                \"per F1\":None,\n",
    "                \"epoch\":None,\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a0b72b6-6d49-4f07-90b3-3a0fd8e474fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 lr?: 0.0906698729810778 error: 0.9863503412414689  train_loss = tensor(77.0225, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, loss = 3.78:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 lr?: 0.09054496737905815 error: 0.9857003574910628  train_loss = tensor(77.7587, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, loss = 1.49:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 lr?: 0.09043227271178698 error: 0.9847253818654533  train_loss = tensor(75.0718, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, loss = 3.58:  54%|█████▍    | 7/13 [00:03<00:03,  1.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 lr?: 0.09033209786751398 error: 0.9863503412414689  train_loss = tensor(77.0461, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, loss = 1.97:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 lr?: 0.09024471741852422 error: 0.9831004224894377  train_loss = tensor(75.5617, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65, loss = 2.61:  54%|█████▍    | 7/13 [00:03<00:03,  1.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 lr?: 0.09017037086855464 error: 0.983750406239844  train_loss = tensor(76.0929, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66, loss = 2.86:  54%|█████▍    | 7/13 [00:03<00:03,  1.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 lr?: 0.09010926199633096 error: 0.9896002599935002  train_loss = tensor(76.3787, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67, loss = 3.60:  54%|█████▍    | 7/13 [00:03<00:03,  1.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 lr?: 0.09006155829702431 error: 0.9899252518687033  train_loss = tensor(77.1675, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68, loss = 5.26:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 lr?: 0.09002739052315863 error: 0.9857003574910628  train_loss = tensor(78.7346, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69, loss = 2.02:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 lr?: 0.09000685232622713 error: 0.9840753981150471  train_loss = tensor(75.8841, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    net.train()\n",
    "    loss_sum = 0\n",
    "    evaluation = []\n",
    "    iter = 0\n",
    "    with tqdm.tqdm(total=iter_per_epoch) as pbar:\n",
    "        error_counter = 0\n",
    "\n",
    "        for X, y in train_iter:\n",
    "            trans = []\n",
    "            #print(\"X shape : {}\".format(X.shape))\n",
    "            #print(\"X shape[3] : {}\".format(X.shape[3]))\n",
    "            for i in range(X.shape[0]):\n",
    "                t1 = transform(X[i], \"crop_resize\")\n",
    "                #print(\"t1 shape : {}\".format(t1.shape))\n",
    "                trans.append(t1)\n",
    "                \n",
    "            for i in range(X.shape[0]):\n",
    "                t2 = transform(X[i], 'permute')\n",
    "                #print(\"t2 shape : {}\".format(t2.shape))\n",
    "                trans.append(t2)\n",
    "                \n",
    "            trans = np.concatenate(trans)\n",
    "            \n",
    "            trans = torch.tensor(trans, dtype=torch.float, device=device)\n",
    "            #print(trans.shape)\n",
    "            output = net(trans)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l, lab_con, log_con = comtrast_loss(output, criterion)\n",
    "            _, log_p = torch.max(log_con.data,1)\n",
    "            evaluation.append((log_p == lab_con).tolist())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += l\n",
    "            iter += 1\n",
    "            pbar.set_description(\"Epoch %d, loss = %.2f\" % (epoch, l.data))\n",
    "            pbar.update(1)\n",
    "        err = l.data\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "\n",
    "\n",
    "    train_acc = sum(evaluation) / len(evaluation)\n",
    "    error = 1 - train_acc\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if epoch % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    print(\"Epoch:\", epoch,\"lr?:\", current_lr, \"error:\", error, \" train_loss =\", loss_sum.data)\n",
    "    scheduler_warmup.step()\n",
    "    state = {\"state_dict\": net.state_dict(), \"epoch\": epoch}\n",
    "    save_ckpt(state, best_err > error, model_save_dir)\n",
    "    best_err = min(best_err, error)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44b9ce-25ca-4732-8844-613139b303fb",
   "metadata": {},
   "source": [
    "### EVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09f07f1a-ff9b-45bc-97af-141d7050216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([512, 2, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x640 and 384x384)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/2883914898.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X shape : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_114/1689235565.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1024x640 and 384x384)"
     ]
    }
   ],
   "source": [
    "net = resnet18(classification=True).to(device)\n",
    "#net = nn.DataParallel(net)\n",
    "checkpoint = torch.load(os.path.join(model_save_dir,'best_w.pth'))\n",
    "net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "epochs_t = 70\n",
    "lr_schduler = CosineAnnealingLR(optimizer, T_max=epochs_t - 10, eta_min=0.09)#default =0.07\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=10, after_scheduler=lr_schduler)\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "scheduler_warmup.step()\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "val_acc_list = []\n",
    "n_train_samples = x_train.shape[0]\n",
    "iter_per_epoch = n_train_samples // batch_size + 1\n",
    "best_acc = -1\n",
    "\n",
    "for epoch in range(epochs_t):\n",
    "    net.train()\n",
    "    loss_sum = 0\n",
    "    evaluation = []\n",
    "    iter = 0\n",
    "    with tqdm.tqdm(total=iter_per_epoch) as pbar:\n",
    "        for X, y in train_iter:\n",
    "            print(\"X shape : {}\".format(X.shape))\n",
    "            output = net(X)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            evaluation.append((predicted == y).tolist())\n",
    "            optimizer.zero_grad()\n",
    "            l = criterion(output, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += l\n",
    "            iter += 1\n",
    "            pbar.set_description(\"Epoch %d, loss = %.2f\" % (epoch, l.data))\n",
    "            pbar.update(1)\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "    train_acc = sum(evaluation) / len(evaluation)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(\"Epoch:\", epoch,\"lr:\", current_lr,\" train_loss =\", loss_sum.data, \" train_acc =\", train_acc)\n",
    "    # scheduler.step()\n",
    "    scheduler_warmup.step()\n",
    "    val_loss = 0\n",
    "    evaluation = []\n",
    "    pred_v = []\n",
    "    true_v = []\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for X, y in test_iter:\n",
    "            output = net(X)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            evaluation.append((predicted == y).tolist())\n",
    "            l = criterion(output, y)\n",
    "            val_loss += l\n",
    "            pred_v.append(predicted.tolist())\n",
    "            true_v.append(y.tolist())\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "    pred_v = [item for sublist in pred_v for item in sublist]\n",
    "    true_v = [item for sublist in true_v for item in sublist]\n",
    "\n",
    "    running_acc = sum(evaluation) / len(evaluation)\n",
    "    val_acc_list.append(running_acc)\n",
    "    print(\"val_loss =\", val_loss, \"val_acc =\", running_acc)\n",
    "\n",
    "\n",
    "    state = {\"state_dict\": net.state_dict(), \"epoch\": epoch}\n",
    "    save_ckpt(state, best_acc < running_acc, model_save_dir, 'best_cls.pth')\n",
    "    best_acc = max(best_acc, running_acc)\n",
    "\n",
    "print(\"Highest acc:\", max(val_acc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126f035-0257-4933-9893-21eb7550ca1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
