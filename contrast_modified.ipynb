{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5c784b-e99a-40e1-9453-a7d754508a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148b7e57-a8b7-4695-ad8e-775cb0fe0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /opt/conda/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from mne) (3.0.3)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/conda/lib/python3.9/site-packages (from mne) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from mne) (1.7.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from mne) (4.62.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from mne) (21.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from mne) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.9/site-packages (from mne) (1.21.5)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from pooch>=1.5->mne) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from pooch>=1.5->mne) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->mne) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->mne) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (4.28.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mne) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mne) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a06ff5a-770e-490f-8b8a-2341de2d96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c352b33-cc7e-47b9-bb02-cc55034373c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: warmup-scheduler in /opt/conda/lib/python3.9/site-packages (0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install warmup-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3fe08b-8f7c-4439-a50a-06ac2d537926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a67bea-12f3-413f-a6ba-9c00854fe16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from net import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import tqdm\n",
    "import mit_utils as utils\n",
    "# import analytics\n",
    "import time\n",
    "import os, shutil\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c5c92a-1dfe-458f-84ae-861d039e98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=64, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        return events , event_id\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def get_X_y(self):\n",
    "        if self.epochs is None:\n",
    "            events , event_id=self.create_epochs()\n",
    "        self.X = self.epochs.get_data()\n",
    "        self.y = self.epochs.events[:, -1]\n",
    "        return self.X, self.y\n",
    "    \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"edf\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_edf( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "        \n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='valid_loss')\n",
    "    plt.title('loss {}'.format(iter))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e1694-ae3c-4a9f-9ab9-19fa54692d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba306842-10d9-4f07-ac2f-f2a39c09f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def save_ckpt(state, is_best, model_save_dir, message='best_w.pth'):\n",
    "    current_w = os.path.join(model_save_dir, 'latest_w.pth')\n",
    "    best_w = os.path.join(model_save_dir, message)\n",
    "    torch.save(state, current_w)\n",
    "    if is_best: shutil.copyfile(current_w, best_w)\n",
    "\n",
    "def transform(x, mode):\n",
    "    x_ = x.cpu().numpy()\n",
    "\n",
    "    Trans = Transform()\n",
    "    if mode == 'time_warp':\n",
    "        pieces = random.randint(5,20)\n",
    "        stretch = random.uniform(1.5,4)\n",
    "        squeeze = random.uniform(0.25,0.67)\n",
    "        x_ = Trans.time_warp(x_, 100, pieces, stretch, squeeze)\n",
    "    elif mode == 'noise':\n",
    "        factor = random.uniform(10,20)\n",
    "        x_ = Trans.add_noise_with_SNR(x_,factor)\n",
    "    elif mode == 'scale':\n",
    "        x_ = Trans.scaled(x_,[0.3,3])\n",
    "    elif mode == 'negate':\n",
    "        x_ = Trans.negate(x_)\n",
    "    elif mode == 'hor_flip':\n",
    "        x_ = Trans.hor_filp(x_)\n",
    "        \n",
    "    elif mode == 'permute':\n",
    "        pieces = random.randint(5,20)\n",
    "        x_ = Trans.permute(x_,pieces)\n",
    "        \n",
    "    elif mode == 'cutout_resize':\n",
    "        pieces = random.randint(5, 20)\n",
    "        x_ = Trans.cutout_resize(x_, pieces)\n",
    "    elif mode == 'cutout_zero':\n",
    "        pieces = random.randint(5, 20)\n",
    "        x_ = Trans.cutout_zero(x_, pieces)\n",
    "    elif mode == 'crop_resize':\n",
    "        size = random.uniform(0.25,0.75)\n",
    "        x_ = Trans.crop_resize(x_, size)\n",
    "    elif mode == 'move_avg':\n",
    "        n = random.randint(3, 10)\n",
    "        x_ = Trans.move_avg(x_,n, mode=\"same\")\n",
    "    #     to test\n",
    "    elif mode == 'lowpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff = random.uniform(5,20)\n",
    "        x_ = Trans.lowpass_filter(x_, order, [cutoff])\n",
    "    elif mode == 'highpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff = random.uniform(5, 10)\n",
    "        x_ = Trans.highpass_filter(x_, order, [cutoff])\n",
    "    elif mode == 'bandpass':\n",
    "        order = random.randint(3, 10)\n",
    "        cutoff_l = random.uniform(1, 5)\n",
    "        cutoff_h = random.uniform(20, 40)\n",
    "        cutoff = [cutoff_l, cutoff_h]\n",
    "        x_ = Trans.bandpass_filter(x_, order, cutoff)\n",
    "\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    x_ = x_.copy()\n",
    "    x_ = x_[:,None,:]\n",
    "    return x_\n",
    "\n",
    "def comtrast_loss(x, criterion):\n",
    "    LARGE_NUM = 1e9\n",
    "    temperature = 0.1\n",
    "    x = F.normalize(x, dim=-1)\n",
    "\n",
    "    num = int(x.shape[0] / 2)\n",
    "    hidden1, hidden2 = torch.split(x, num)\n",
    "\n",
    "\n",
    "    hidden1_large = hidden1\n",
    "    hidden2_large = hidden2\n",
    "    labels = torch.arange(0,num).to(device)\n",
    "    masks = F.one_hot(torch.arange(0,num), num).to(device)\n",
    "\n",
    "\n",
    "    logits_aa = torch.matmul(hidden1, hidden1_large.T) / temperature\n",
    "    logits_aa = logits_aa - masks * LARGE_NUM\n",
    "    logits_bb = torch.matmul(hidden2, hidden2_large.T) / temperature\n",
    "    logits_bb = logits_bb - masks * LARGE_NUM\n",
    "    logits_ab = torch.matmul(hidden1, hidden2_large.T) / temperature\n",
    "    logits_ba = torch.matmul(hidden2, hidden1_large.T) / temperature\n",
    "    # print(labels)\n",
    "    #\n",
    "    # print(torch.cat([logits_ab, logits_aa], 1).shape)\n",
    "\n",
    "    loss_a = criterion(torch.cat([logits_ab, logits_aa], 1),\n",
    "        labels)\n",
    "    loss_b = criterion(torch.cat([logits_ba, logits_bb], 1),\n",
    "        labels)\n",
    "    loss = torch.mean(loss_a + loss_b)\n",
    "    return loss, labels, logits_ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e20e1b7-3be3-4c2e-bd06-ae16476baacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed1d.pth',\n",
    "}\n",
    "\n",
    "dp_rate = 0\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=33, stride=stride,\n",
    "                     padding=16, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes*2)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn0(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "            # residual = torch.cat((residual,residual),1)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(inplanes)\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=33, bias=False, padding=16)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=65, stride=stride,\n",
    "                               padding=32, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn0(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        # out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "            # residual = torch.cat((residual, residual), 1)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, classification, num_classes=2):\n",
    "        self.inplanes = 12\n",
    "        self.classification = classification\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(2, self.inplanes, kernel_size=33, stride=1, padding=16,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(self.inplanes, self.inplanes, kernel_size=33, stride=2, padding=16,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(self.inplanes)\n",
    "        self.downsample = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(self.inplanes, self.inplanes, kernel_size=33, stride=1, padding=16,\n",
    "                               bias=False)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=2)\n",
    "        # self.layer5 = self._make_layer(block, self.inplanes, layers[4], stride=2)\n",
    "        self.bn_final = nn.BatchNorm1d(96*2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(2)\n",
    "        self.fc1 = nn.Linear(716, 384)\n",
    "        self.bn3 = nn.BatchNorm1d(384)\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.bn4 = nn.BatchNorm1d(192)\n",
    "        self.fc3 = nn.Linear(192, 5)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        out = self.conv2(x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv3(out)\n",
    "        residual = self.downsample(x)\n",
    "        #print('residual : {}'.format(residual.shape))\n",
    "        #print('out : {}'.format(out.shape))\n",
    "        out += residual\n",
    "        x = self.relu(out)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # x = self.layer5(x)\n",
    "        x = self.bn_final(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(\"x shape : {}\".format(x.shape))\n",
    "        if self.classification:\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            # x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [ 2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a1ed4f-a45c-4f32-ae7d-a7b002901dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_noise(self, signal, noise_amount):\n",
    "        \"\"\"\n",
    "        adding noise\n",
    "        \"\"\"\n",
    "        signal = signal.T\n",
    "        noise = (0.4 ** 0.5) * np.random.normal(1, noise_amount, np.shape(signal)[0])\n",
    "        noise = noise[:,None]\n",
    "        noised_signal = signal + noise\n",
    "        noised_signal = noised_signal.T\n",
    "        # print(noised_signal.shape)\n",
    "        return noised_signal\n",
    "\n",
    "    def add_noise_with_SNR(self,signal, noise_amount):\n",
    "        \"\"\"\n",
    "        adding noise\n",
    "        created using: https://stackoverflow.com/a/53688043/10700812\n",
    "        \"\"\"\n",
    "        signal = signal[0]\n",
    "        target_snr_db = noise_amount  # 20\n",
    "        x_watts = signal ** 2  # Calculate signal power and convert to dB\n",
    "        sig_avg_watts = np.mean(x_watts)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)  # Calculate noise then convert to watts\n",
    "        noise_avg_db = sig_avg_db - target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts),\n",
    "                                       len(x_watts))  # Generate an sample of white noise\n",
    "        noised_signal = signal + noise_volts  # noise added signal\n",
    "        noised_signal = noised_signal[None,:]\n",
    "        # print(noised_signal.shape)\n",
    "\n",
    "        return noised_signal\n",
    "\n",
    "    def scaled(self,signal, factor_list):\n",
    "        \"\"\"\"\n",
    "        scale the signal\n",
    "        \"\"\"\n",
    "        factor = round(np.random.uniform(factor_list[0],factor_list[1]),2)\n",
    "        signal[0] = 1 / (1 + np.exp(-signal[0]))\n",
    "        # print(signal.max())\n",
    "        return signal\n",
    "\n",
    "    def negate(self,signal):\n",
    "        \"\"\"\n",
    "        negate the signal\n",
    "        \"\"\"\n",
    "        signal[0] = signal[0] * (-1)\n",
    "        return signal\n",
    "\n",
    "    def hor_filp(self,signal):\n",
    "        \"\"\"\n",
    "        flipped horizontally\n",
    "        \"\"\"\n",
    "        hor_flipped = np.flip(signal,axis=1)\n",
    "        return hor_flipped\n",
    "\n",
    "\n",
    "\n",
    "    def cutout_resize(self,signal,pieces):\n",
    "        \"\"\"\n",
    "                signal: numpy array (batch x window)\n",
    "                pieces: number of segments along time\n",
    "                cutout 1 piece\n",
    "                \"\"\"\n",
    "        signal = signal.T\n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist())  # 向上取整\n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "        import random\n",
    "        sequence = []\n",
    "\n",
    "        cutout = random.randint(0, pieces)\n",
    "        # print(cutout)\n",
    "        # sequence1 = list(range(0, cutout))\n",
    "        # sequence2 = list(range(int(cutout + 1), pieces))\n",
    "        # sequence = np.hstack((sequence1, sequence2))\n",
    "        for i in range(pieces):\n",
    "            if i == cutout:\n",
    "                pass\n",
    "            else:\n",
    "                sequence.append(i)\n",
    "        # print(sequence)\n",
    "\n",
    "        cutout_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                     (pieces, piece_length)).tolist()\n",
    "\n",
    "        tail = signal[(np.shape(signal)[0] // pieces * pieces):]\n",
    "\n",
    "        cutout_signal = np.asarray(cutout_signal)[sequence]\n",
    "\n",
    "        cutout_signal = np.hstack(cutout_signal)\n",
    "        cutout_signal = np.concatenate((cutout_signal, tail[:, 0]), axis=0)\n",
    "\n",
    "        cutout_signal = cv2.resize(cutout_signal, (1, 3072), interpolation=cv2.INTER_LINEAR)\n",
    "        cutout_signal = cutout_signal.T\n",
    "\n",
    "\n",
    "        return cutout_signal\n",
    "\n",
    "    def cutout_zero(self,signal,pieces):\n",
    "        \"\"\"\n",
    "                signal: numpy array (batch x window)\n",
    "                pieces: number of segments along time\n",
    "                cutout 1 piece\n",
    "                \"\"\"\n",
    "        signal = signal.T\n",
    "        ones = np.ones((np.shape(signal)[0],np.shape(signal)[1]))\n",
    "        # print(ones.shape)\n",
    "        # assert False\n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist())  # 向上取整\n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "\n",
    "\n",
    "        cutout = random.randint(1, pieces)\n",
    "        cutout_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                     (pieces, piece_length)).tolist()\n",
    "        ones_pieces = np.reshape(ones[:(np.shape(signal)[0] // pieces * pieces)],\n",
    "                                   (pieces, piece_length)).tolist()\n",
    "        tail = signal[(np.shape(signal)[0] // pieces * pieces):]\n",
    "\n",
    "        cutout_signal = np.asarray(cutout_signal)\n",
    "        ones_pieces = np.asarray(ones_pieces)\n",
    "        for i in range(pieces):\n",
    "            if i == cutout:\n",
    "                ones_pieces[i]*=0\n",
    "\n",
    "        cutout_signal = cutout_signal * ones_pieces\n",
    "        cutout_signal = np.hstack(cutout_signal)\n",
    "        cutout_signal = np.concatenate((cutout_signal, tail[:, 0]), axis=0)\n",
    "        cutout_signal = cutout_signal[:,None]\n",
    "        cutout_signal = cutout_signal.T\n",
    "\n",
    "        return cutout_signal\n",
    "    # mic\n",
    "    \n",
    "\n",
    "    def move_avg(self,a,n, mode=\"same\"):\n",
    "        # a = a.T\n",
    "\n",
    "        result = np.convolve(a[0], np.ones((n,)) / n, mode=mode)\n",
    "        return result[None,:]\n",
    "\n",
    "    def bandpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        w2 = 2 * cutoff[1] / int(fs)\n",
    "        b, a = signal.butter(order, [w1, w2], btype='bandpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def lowpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        # w2 = 2 * cutoff[1] / fs\n",
    "        b, a = signal.butter(order, w1, btype='lowpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def highpass_filter(self, x, order, cutoff, fs=100):\n",
    "        result = np.zeros((x.shape[0], x.shape[1]))\n",
    "        w1 = 2 * cutoff[0] / int(fs)\n",
    "        # w2 = 2 * cutoff[1] / fs\n",
    "        b, a = signal.butter(order, w1, btype='highpass')  # 配置滤波器 8 表示滤波器的阶数\n",
    "        result = signal.filtfilt(b, a, x, axis=1)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def time_warp(self,signal, sampling_freq, pieces, stretch_factor, squeeze_factor):\n",
    "        \"\"\"\n",
    "        signal: numpy array (batch x window)\n",
    "        sampling freq\n",
    "        pieces: number of segments along time\n",
    "        stretch factor\n",
    "        squeeze factor\n",
    "        \"\"\"\n",
    "        signal = signal.T\n",
    "\n",
    "        total_time = np.shape(signal)[0] // sampling_freq\n",
    "        segment_time = total_time / pieces\n",
    "        sequence = list(range(0, pieces))\n",
    "        stretch = np.random.choice(sequence, math.ceil(len(sequence) / 2), replace=False)\n",
    "        squeeze = list(set(sequence).difference(set(stretch)))\n",
    "        initialize = True\n",
    "        for i in sequence:\n",
    "            orig_signal = signal[int(i * np.floor(segment_time * sampling_freq)):int(\n",
    "                (i + 1) * np.floor(segment_time * sampling_freq))]\n",
    "            orig_signal = orig_signal.reshape(np.shape(orig_signal)[0],64, 1)\n",
    "            if i in stretch:\n",
    "                output_shape = int(np.ceil(np.shape(orig_signal)[0] * stretch_factor))\n",
    "                new_signal = cv2.resize(orig_signal, (1, output_shape), interpolation=cv2.INTER_LINEAR)\n",
    "                if initialize == True:\n",
    "                    time_warped = new_signal\n",
    "                    initialize = False\n",
    "                else:\n",
    "                    time_warped = np.vstack((time_warped, new_signal))\n",
    "            elif i in squeeze:\n",
    "                output_shape = int(np.ceil(np.shape(orig_signal)[0] * squeeze_factor))\n",
    "                new_signal = cv2.resize(orig_signal, (1, output_shape), interpolation=cv2.INTER_LINEAR)\n",
    "                if initialize == True:\n",
    "                    time_warped = new_signal\n",
    "                    initialize = False\n",
    "                else:\n",
    "                    time_warped = np.vstack((time_warped, new_signal))\n",
    "        time_warped = cv2.resize(time_warped, (1,3072), interpolation=cv2.INTER_LINEAR)\n",
    "        time_warped = time_warped.T\n",
    "        return time_warped\n",
    "    \n",
    "    \n",
    "    def crop_resize(self, signal, size):\n",
    "        #print(signal.shape)\n",
    "        \n",
    "        signal = signal.T\n",
    "        size = signal.shape[0] * size\n",
    "        size = int(size)\n",
    "        start = random.randint(0, signal.shape[0]-size)\n",
    "        crop_signal = signal[start:start + size,:]\n",
    "        #print(crop_signal.shape)\n",
    "\n",
    "        crop_signal = cv2.resize(crop_signal, (2, 640), interpolation=cv2.INTER_LINEAR)\n",
    "        # print(crop_signal.shape)\n",
    "        crop_signal = crop_signal.T\n",
    "        #print(\"crop_signal.shape : {}\".format(crop_signal.shape))\n",
    "        return crop_signal\n",
    "    \n",
    "    \n",
    "    def permute(self,signal, pieces):\n",
    "        \"\"\"\n",
    "        signal: numpy array (batch x window)\n",
    "        pieces: number of segments along time\n",
    "        \"\"\"\n",
    "        #print('signal shape ; {}'.format(signal.shape))\n",
    "        signal = signal.T\n",
    "        \n",
    "        pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist()) #向上取整\n",
    "        piece_length = int(np.shape(signal)[0] // pieces)\n",
    "        #print(pieces*piece_length)\n",
    "        cal = pieces*piece_length\n",
    "        while cal != 640:\n",
    "            pieces = random.randint(5,20)\n",
    "            pieces = int(np.ceil(np.shape(signal)[0] / (np.shape(signal)[0] // pieces)).tolist()) #向上取整\n",
    "            piece_length = int(np.shape(signal)[0] // pieces)\n",
    "            #print(pieces*piece_length)\n",
    "            cal = pieces*piece_length\n",
    "            \n",
    "        sequence = list(range(0, pieces))\n",
    "        np.random.shuffle(sequence)\n",
    "        #print(signal[:(np.shape(signal)[0] // pieces * pieces)].shape)\n",
    "        for i in range(signal.shape[1]):\n",
    "            #print(i)\n",
    "            #print('signal shape loop ; {}'.format(signal.shape))\n",
    "            # 2,640\n",
    "            permuted_signal = np.reshape(signal[:(np.shape(signal)[0] // pieces * pieces),i],\n",
    "                                         (pieces, piece_length)).tolist()\n",
    "            #print('permuted_signal : {}'.format(len(permuted_signal)))\n",
    "            tail = signal[i,(np.shape(signal)[0] // pieces * pieces):]\n",
    "            \n",
    "            #print('tail shape  ; {}'.format(tail.shape))\n",
    "            permuted_signal = np.asarray(permuted_signal)[sequence]\n",
    "            permuted_signal = np.concatenate(permuted_signal, axis=0)\n",
    "            #print('permuted_signal shape  ; {}'.format(permuted_signal.shape))\n",
    "            permuted_signal = np.concatenate((permuted_signal,tail), axis=0)\n",
    "            permuted_signal = permuted_signal[:,None]\n",
    "            permuted_signal = permuted_signal.T\n",
    "            if i == 0 :\n",
    "                permuted_signal_re = permuted_signal\n",
    "            else:\n",
    "                permuted_signal_re = np.stack((permuted_signal_re,permuted_signal))\n",
    "            #print(permuted_signal_re.shape)\n",
    "        return permuted_signal_re\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e917af6-f1c7-4361-b4ec-ace0d684544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Apply filter.\n",
      "Filtering raw data in 474 contiguous segments\n",
      "Setting up band-pass filter from 0.05 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.05\n",
      "- Lower transition bandwidth: 0.05 Hz (-6 dB cutoff frequency: 0.03 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 10561 samples (66.006 sec)\n",
      "\n",
      ">>> Create Epochs.\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "7110 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7110 events and 641 original time points ...\n",
      "43 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[    672,       0,       1],\n",
       "        [   2000,       0,       0],\n",
       "        [   3328,       0,       0],\n",
       "        ...,\n",
       "        [9355488,       0,       1],\n",
       "        [9356816,       0,       0],\n",
       "        [9358144,       0,       1]]),\n",
       " {'T1': 0, 'T2': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = '1.0.0'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "runs = [3, 4, 7, 8, 11, 12]\n",
    "subjects = [i for i in range(1, 80)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "# apply filter\n",
    "freq = (0.05, 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.raw_ica()\n",
    "eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcca7d47-2ced-4852-81c9-d4e921b68fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7067, 64, 641) (7067,)\n",
      "(7067, 1, 641)\n",
      "(7067, 1, 641)\n",
      "(7067, 2, 641)\n",
      "(7067, 2, 640)\n"
     ]
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    " \n",
    "#X = X[:, np.newaxis,:,:]\n",
    "X.shape\n",
    "\n",
    "X2 = X[:,  7:8, :] \n",
    "print(X2.shape)\n",
    "\n",
    "X3= X[:,  13:14, :]\n",
    "print(X3.shape)\n",
    "X4 = np.concatenate((X2,X3), axis=1)\n",
    "print(X4.shape)\n",
    "X = X4\n",
    "X=X[:,:,:640]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aead17ed-830d-4cdb-86f7-0295d2dfceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bdda6f-001a-4baf-8f15-0417d4f44c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(classification=False).to(device)\n",
    "#net = nn.DataParallel(net).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "batch_size = 512\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1 * (batch_size / 64), momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "epochs = 1000\n",
    "lr_schduler = CosineAnnealingLR(optimizer, T_max=epochs - 10, eta_min=0.05)#default =0.07\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=10, after_scheduler=lr_schduler)\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "scheduler_warmup.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc60be98-3102-47a7-9f7b-00cfe85d1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4946, 2, 640) (4946,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "      \n",
    "x_train = torch.tensor(x_train, dtype=torch.float).to(device)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c0f955-fa2f-44b1-a70e-c406ab100845",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = ['R', 'L']\n",
    "val_acc_list = []\n",
    "n_train_samples = x_train.shape[0]\n",
    "iter_per_epoch = n_train_samples // batch_size + 1\n",
    "best_acc = -1\n",
    "err = []\n",
    "best_err = 1\n",
    "margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed86e63-47b8-407c-85e3-d68729c195f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "model_name = 'resnet17'\n",
    "model_save_dir = '%s/%s_%s' % (log_dir, model_name, time.strftime(\"%m%d%H%M\"))\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "log_templete = {\"acc\": None,\n",
    "                    \"cm\": None,\n",
    "                    \"f1\": None,\n",
    "                \"per F1\":None,\n",
    "                \"epoch\":None,\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe07ef3a-e3be-4ee3-9b19-e91a4fcaee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot_acc_loss(acc, loss):\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(acc, label='acc')\n",
    "    plt.plot(loss, label='loss')\n",
    "    #plt.title('loss {}'.format(iter))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b72b6-6d49-4f07-90b3-3a0fd8e474fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([1024, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 13.18:  10%|█         | 1/10 [00:00<00:06,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([1024, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 12.19:  20%|██        | 2/10 [00:01<00:05,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([1024, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 12.01:  30%|███       | 3/10 [00:01<00:04,  1.59it/s]"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "acc =[]\n",
    "loss_train = [] \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    net.train()\n",
    "    loss_sum = 0\n",
    "    evaluation = []\n",
    "    \n",
    "    iter = 0\n",
    "    with tqdm.tqdm(total=iter_per_epoch) as pbar:\n",
    "        error_counter = 0\n",
    "        \n",
    "        for X, y in train_iter:\n",
    "            trans1 = np.zeros(X.shape)\n",
    "            trans2 = np.zeros(X.shape)\n",
    "            #print(\"trans2 : {}\".format(trans2.shape))\n",
    "            #print(\"X shape : {}\".format(X.shape))\n",
    "            #print(\"X shape[3] : {}\".format(X.shape[3]))\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                t1 = transform(X[i], \"crop_resize\")\n",
    "                #print(\"t1 shape : {}\".format(t1.shape))\n",
    "                trans1[i] = t1.reshape(2,X.shape[2])\n",
    "            #print(\"trans1 shape : {}\".format(trans1))   \n",
    "            for i in range(X.shape[0]):\n",
    "                t2 = transform(X[i], 'permute')\n",
    "                \n",
    "                #print(\"t2 shape : {}\".format(t2.shape))\n",
    "                trans2[i] = t2.reshape(2,X.shape[2])\n",
    "                \n",
    "            trans = np.concatenate((trans1,trans2))\n",
    "            \n",
    "            trans = torch.tensor(trans, dtype=torch.float, device=device)\n",
    "            \n",
    "            output = net(trans)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l, lab_con, log_con = comtrast_loss(output, criterion)\n",
    "            _, log_p = torch.max(log_con.data,1)\n",
    "            evaluation.append((log_p == lab_con).tolist())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += l\n",
    "            iter += 1\n",
    "            pbar.set_description(\"Epoch %d, loss = %.2f\" % (epoch, l.data))\n",
    "            pbar.update(1)\n",
    "        err = l.data\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "\n",
    "\n",
    "    train_acc = sum(evaluation) / len(evaluation)\n",
    "    error = 1 - train_acc\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if epoch % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    acc.append(train_acc*100)\n",
    "    loss_train.append(loss_sum.data.cpu())\n",
    "    #print(acc)\n",
    "    #print(loss_train)\n",
    "    do_plot_acc_loss(acc, loss_train)\n",
    "    print(\"Epoch:\", epoch,\"lr?:\", current_lr, \"error:\", error, \" train_loss =\", loss_sum.data, \"Acc =\",train_acc*100)\n",
    "    #do_plot(loss_sum.data, error)\n",
    "    scheduler_warmup.step()\n",
    "    state = {\"state_dict\": net.state_dict(), \"epoch\": epoch}\n",
    "    save_ckpt(state, best_err > error, model_save_dir)\n",
    "    best_err = min(best_err, error)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44b9ce-25ca-4732-8844-613139b303fb",
   "metadata": {},
   "source": [
    "### EVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f07f1a-ff9b-45bc-97af-141d7050216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(classification=True).to(device)\n",
    "#net = nn.DataParallel(net)\n",
    "checkpoint = torch.load(os.path.join(\"logs/resnet17_05160848\",'best_w.pth'))\n",
    "net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "epochs_t = 70\n",
    "lr_schduler = CosineAnnealingLR(optimizer, T_max=epochs_t - 10, eta_min=0.09)#default =0.07\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=10, after_scheduler=lr_schduler)\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "scheduler_warmup.step()\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "val_acc_list = []\n",
    "n_train_samples = x_train.shape[0]\n",
    "iter_per_epoch = n_train_samples // batch_size + 1\n",
    "best_acc = -1\n",
    "\n",
    "for epoch in range(epochs_t):\n",
    "    net.train()\n",
    "    loss_sum = 0\n",
    "    evaluation = []\n",
    "    iter = 0\n",
    "    with tqdm.tqdm(total=iter_per_epoch) as pbar:\n",
    "        for X, y in train_iter:\n",
    "            #print(\"X shape : {}\".format(X.shape))\n",
    "            output = net(X)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            evaluation.append((predicted == y).tolist())\n",
    "            optimizer.zero_grad()\n",
    "            l = criterion(output, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += l\n",
    "            iter += 1\n",
    "            pbar.set_description(\"Epoch %d, loss = %.2f\" % (epoch, l.data))\n",
    "            pbar.update(1)\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "    train_acc = sum(evaluation) / len(evaluation)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(\"Epoch:\", epoch,\"lr:\", current_lr,\" train_loss =\", loss_sum.data, \" train_acc =\", train_acc)\n",
    "    # scheduler.step()\n",
    "    scheduler_warmup.step()\n",
    "    val_loss = 0\n",
    "    evaluation = []\n",
    "    pred_v = []\n",
    "    true_v = []\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for X, y in test_iter:\n",
    "            output = net(X)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            evaluation.append((predicted == y).tolist())\n",
    "            l = criterion(output, y)\n",
    "            val_loss += l\n",
    "            pred_v.append(predicted.tolist())\n",
    "            true_v.append(y.tolist())\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "    pred_v = [item for sublist in pred_v for item in sublist]\n",
    "    true_v = [item for sublist in true_v for item in sublist]\n",
    "\n",
    "    running_acc = sum(evaluation) / len(evaluation)\n",
    "    val_acc_list.append(running_acc)\n",
    "    print(\"val_loss =\", val_loss, \"val_acc =\", running_acc)\n",
    "\n",
    "\n",
    "    state = {\"state_dict\": net.state_dict(), \"epoch\": epoch}\n",
    "    save_ckpt(state, best_acc < running_acc, model_save_dir, 'best_cls.pth')\n",
    "    best_acc = max(best_acc, running_acc)\n",
    "\n",
    "print(\"Highest acc:\", max(val_acc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126f035-0257-4933-9893-21eb7550ca1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe8263-2b0f-473e-b4e1-1c1f57502866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a06601-f3d9-406b-b31e-a3e80f6abaca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
